{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('py37_pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b5079757533a07d523b22964b5bda6a42550d36e9c94eb5f662e38c34376cbf3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Prepare data\n",
    "\n",
    "Load and generate train_X, train_Y, test_X, test_Y, then convert to ndarray and save.\n",
    "\n",
    "numeric feature in float type, category feature in object(str) type.\n",
    "\n",
    "Generate cat_cols list, then convert to int and save.\n",
    "\n",
    "Prepare Adult, Amazon, Click prediction, KDD appetency, KDD churn, KDD internet, KDD upselling, KDD 98, Kick prediction dataset mainly according to catboost benchmark: https://github.com/catboost/benchmarks/tree/master/quality_benchmarks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "import re \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult, Amazon, Click prediction, KDD appetency, KDD churn, KDD internet, KDD upselling, KDD 98, Kick prediction\n",
    "dataset = 'KDD appetency'  \n",
    "dataset_path = \"/home/v-tyan/NN_for_tabular/datasets_raw/\""
   ]
  },
  {
   "source": [
    "## Adult"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'Adult':\n",
    "    cols = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "    target_col = 'income'\n",
    "    cat_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "    num_cols = list(set(cols) - {target_col} - set(cat_cols))\n",
    "    train_df = pd.read_csv(os.path.join(dataset_path, 'Adult/adult.data'), sep=', ', header=None, names=cols, na_values='?', engine='python')\n",
    "    test_df = pd.read_csv(os.path.join(dataset_path, 'Adult/adult.test'), sep=', ', header=None, names=cols, na_values='?', engine='python')\n",
    "    test_df.replace({'<=50K.': '<=50K', '>50K.': '>50K'}, inplace=True)\n",
    "    train_X, train_Y = train_df.drop(target_col, axis=1), train_df[target_col]\n",
    "    test_X, test_Y = test_df.drop(target_col, axis=1), test_df[target_col]"
   ]
  },
  {
   "source": [
    "## Amazon"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'Amazon':\n",
    "    df = pd.read_csv(os.path.join(dataset_path, 'Amazon/train.csv'))\n",
    "    cols = list(df.columns)\n",
    "    target_col = 'ACTION'\n",
    "    cat_cols = list(set(set(cols) - {target_col}))\n",
    "    num_cols = []  # assume all are categorial\n",
    "    X, Y = df.drop(target_col, axis=1), df[target_col]\n",
    "    train_idx = pd.read_csv(os.path.join(dataset_path, \"Amazon/stratified_train_idx.txt\"), header=None)\n",
    "    test_idx = pd.read_csv(os.path.join(dataset_path, \"Amazon/stratified_test_idx.txt\"), header=None)\n",
    "    train_X, test_X, train_Y, test_Y = X.iloc[train_idx[0]], X.iloc[test_idx[0]], Y.iloc[train_idx[0]], Y.iloc[test_idx[0]]"
   ]
  },
  {
   "source": [
    "## Click prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'Click prediction':\n",
    "    cols = ['click', 'impression', 'url_hash', 'ad_id', 'advertiser_id', 'depth', 'position', 'query_id', 'keyword_id', 'title_id', 'description_id', 'user_id']\n",
    "    target_col = 'click'\n",
    "    cat_cols = ['impression', 'url_hash', 'ad_id', 'position', 'query_id', 'keyword_id', 'title_id', 'description_id']\n",
    "    num_cols = list(set(cols) - {target_col} - set(cat_cols))\n",
    "    with open(os.path.join(dataset_path, \"Click prediction/track2/subsampling_idx.txt\")) as fin:\n",
    "        ids = list(map(int, fin.read().split()))\n",
    "    unique_ids = set(ids)\n",
    "    data_strings = {}\n",
    "    with open(os.path.join(dataset_path, \"Click prediction/track2/training.txt\")) as fin:\n",
    "        for i, string in enumerate(fin):\n",
    "            if i in unique_ids:\n",
    "                data_strings[i] = string\n",
    "    data_rows = []\n",
    "    for i in ids:\n",
    "        data_rows.append(data_strings[i])\n",
    "    df = pd.read_table(StringIO(\"\".join(data_rows)), header=None, names=cols)    \n",
    "    X, Y = df.drop(target_col, axis=1), df[target_col].apply(lambda x: 1 if x == 0 else -1)\n",
    "    def clean_string(s):\n",
    "        return \"v_\" + re.sub('[^A-Za-z0-9]+', \"_\", str(s))\n",
    "    for cat_col in cat_cols:\n",
    "        X[cat_col] = X[cat_col].apply(clean_string)\n",
    "    train_idx = pd.read_csv(os.path.join(dataset_path, \"Click prediction/track2/stratified_train_idx.txt\"), header=None)\n",
    "    test_idx = pd.read_csv(os.path.join(dataset_path, \"Click prediction/track2/stratified_test_idx.txt\"), header=None)\n",
    "    train_X, test_X, train_Y, test_Y = X.iloc[train_idx[0]], X.iloc[test_idx[0]], Y.iloc[train_idx[0]], Y.iloc[test_idx[0]]"
   ]
  },
  {
   "source": [
    "## KDD appetency, churn, upselling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset in {'KDD appetency', 'KDD churn', 'KDD upselling'}:\n",
    "    df = pd.read_csv(os.path.join(dataset_path, \"appetency_churn_upselling/orange_small_train.data\"), sep = \"\\t\")\n",
    "    cols = list(df.columns)\n",
    "    cat_cols = [cols[idx] for idx in [190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
    "                207, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]]\n",
    "    num_cols = list(set(cols) - set(cat_cols))\n",
    "    dataset_ = dataset.split(' ')[-1]\n",
    "    X, Y = df, -pd.read_csv(os.path.join(dataset_path, f'appetency_churn_upselling/orange_small_train_{dataset_}.labels'), header=None)[0]\n",
    "    train_idx = pd.read_csv(os.path.join(dataset_path, f'appetency_churn_upselling/{dataset_}/stratified_train_idx_{dataset_}.txt'), header=None)\n",
    "    test_idx = pd.read_csv(os.path.join(dataset_path, f'appetency_churn_upselling/{dataset_}/stratified_test_idx_{dataset_}.txt'), header=None)\n",
    "    train_X, test_X, train_Y, test_Y = X.iloc[train_idx[0]], X.iloc[test_idx[0]], Y.iloc[train_idx[0]], Y.iloc[test_idx[0]]"
   ]
  },
  {
   "source": [
    "## Prepare numerical features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[num_cols], test_X[num_cols] = train_X[num_cols].astype(float), test_X[num_cols].astype(float)"
   ]
  },
  {
   "source": [
    "## Prepare category features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_col in cat_cols:\n",
    "    train_X[cat_col] = train_X[cat_col].apply(str)\n",
    "    test_X[cat_col] = test_X[cat_col].apply(str)"
   ]
  },
  {
   "source": [
    "## Convert to ndarray and int"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset in {'Adult', 'Amazon', 'Click prediction', 'KDD appetency', 'KDD churn', 'KDD upselling'}:\n",
    "    cat_cols_ = []\n",
    "    for idx, col in enumerate(train_X.columns):\n",
    "        if col in cat_cols:\n",
    "            cat_cols_.append(idx)\n",
    "    cat_cols = cat_cols_\n",
    "    train_X, train_Y, test_X, test_Y = train_X.values, train_Y.values, test_X.values, test_Y.values"
   ]
  },
  {
   "source": [
    "## Save"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "data = train_X, train_Y, test_X, test_Y, cat_cols\n",
    "save_dir = f\"/home/v-tyan/NN_for_tabular/datasets/{dataset}.npy\"\n",
    "# np.save(save_dir, data)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 31,
   "outputs": []
  }
 ]
}